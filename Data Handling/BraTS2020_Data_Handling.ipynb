{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BraTS 2020 Dataset Handling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brain Tumor Segmentation (BraTS) 2020 dataset is a collection of multimodal Magnetic Resonance Imaging (MRI) scans used for the segmentation of brain tumors.\n",
    "\n",
    "It includes MRI scans from glioma patients, providing four different MRI modalities (means 4 channels of information - 4 different volumes of the same region) per patient:\n",
    "\n",
    "1. **Native (T1)**\n",
    "2. **Post-contrast T1-weighted (T1ce - contrast enhanced)**\n",
    "3. **T2-weighted (T2)**\n",
    "4. **T2-FLAIR (T2 - Fluied Attenuated Inversion Recovery)**\n",
    "\n",
    "These scans come with expert-annotated segmentation masks that delineate the tumor into various sub-regions, such as the necrotic and non-enhancing tumor core, the peritumoral edema, and the enhancing tumor.\n",
    "\n",
    "Annotations (labels):\n",
    "1. **Label 0:** Not Tumor (NT) volume\n",
    "2. **Label 1:** Necrotic and non-enhancing tumor core (NCR/NET)\n",
    "3. **Label 2:** Peritumoral edema (ED)\n",
    "4. **Label 3:** Missing (No pixels in all the volumes contain label 3)\n",
    "5. **Label 4:** GD-enhancing tumor (ET)\n",
    "\n",
    "As there are no pixels with the label 3, we will be replacing label 3 with label 4 so that there is continuity between the labels.\n",
    "\n",
    "Inside the ``brats20-dataset-training-validation`` folder, there are two datasets: one for training and one for validation. The ``BraTS2020_TrainingData`` folder contains another folder with 369 samples (which are patients here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading and Unzipping Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *VSCode (Rockfish)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! cp /home/en520-idev1/project/kaggle.json ~/.kaggle/ # Change path to path of your kaggle.json api file\n",
    "! chmod 600 ~/.kaggle/kaggle.json  \n",
    "!kaggle datasets download -d awsaf49/brats20-dataset-training-validation -p /home/en520-idev1/project  # Change to your own username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /home/en520-idev1/project/brats20-dataset-training-validation.zip -d /home/en520-idev1/project  # Change to your own username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Path = \"/home/en520-idev1/project/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Google Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/ # Change path to path of your kaggle.json api file\n",
    "! chmod 600 ~/.kaggle/kaggle.json \n",
    "! kaggle datasets download -d awsaf49/brats20-dataset-training-validation -p /content/drive/MyDrive  # Change to preferred download path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip /content/drive/MyDrive/brats20-dataset-training-validation.zip -d /content/drive/MyDrive # Change to zip file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Path = \"/content/drive/MyDrive/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load and Explore Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "import PIL\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "from skimage.util import montage\n",
    "import skimage.transform as skTrans\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import resize\n",
    "from PIL import Image, ImageOps\n",
    "import nibabel as nib\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one file that needs to be renamed as it does not match the naming structure of the rest of the files\n",
    "old_name = Train_Path + \"BraTS20_Training_355/W39_1998.09.19_Segm.nii\"\n",
    "new_name = Train_Path + \"BraTS20_Training_355/BraTS20_Training_355_seg.nii\"\n",
    "\n",
    "# renaming the file\n",
    "try:\n",
    "    os.rename(old_name, new_name)\n",
    "    print(\"File has been re-named successfully!\")\n",
    "except:\n",
    "    print(\"File is already renamed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .nii file as a numpy array\n",
    "test_image_flair = nib.load(Train_Path + \"BraTS20_Training_355/BraTS20_Training_355_flair.nii\").get_fdata()\n",
    "print(\"Shape: \", test_image_flair.shape)\n",
    "print(\"Dtype: \", test_image_flair.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the test_image_flair array and then reshape it back to its original dimensions.\n",
    "# This ensures the data is normalized/standardized for model input without altering its spatial structure.\n",
    "scaler = MinMaxScaler()\n",
    "test_image_flair = scaler.fit_transform(test_image_flair.reshape(-1, test_image_flair.shape[-1])).reshape(test_image_flair.shape)\n",
    "\n",
    "# rescaling t1\n",
    "test_image_t1 = nib.load(Train_Path + 'BraTS20_Training_355/BraTS20_Training_355_t1.nii').get_fdata()\n",
    "test_image_t1 = scaler.fit_transform(test_image_t1.reshape(-1, test_image_t1.shape[-1])).reshape(test_image_t1.shape)\n",
    "# rescaling t1ce\n",
    "test_image_t1ce = nib.load(Train_Path + 'BraTS20_Training_355/BraTS20_Training_355_t1ce.nii').get_fdata()\n",
    "test_image_t1ce = scaler.fit_transform(test_image_t1ce.reshape(-1, test_image_t1ce.shape[-1])).reshape(test_image_t1ce.shape)\n",
    "# rescaling t2\n",
    "test_image_t2 = nib.load(Train_Path + 'BraTS20_Training_355/BraTS20_Training_355_t2.nii').get_fdata()\n",
    "test_image_t2 = scaler.fit_transform(test_image_t2.reshape(-1, test_image_t2.shape[-1])).reshape(test_image_t2.shape)\n",
    "# we will not rescale the mask\n",
    "test_image_seg = nib.load(Train_Path + 'BraTS20_Training_355/BraTS20_Training_355_seg.nii').get_fdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four imaging modalities provide distinct perspectives on the same brain image, each highlighting different features.\n",
    "\n",
    "**Detailed Description of Each Modality:**\n",
    "1. **Native (T1):** This modality reveals the structure and composition of various tissue types in the brain. It's instrumental in identifying tumors, cysts, and other abnormalities.\n",
    "2. **Post-contrast T1-weighted (T1ce, also known as T1Gd):** Similar to T1 images, but enhanced with a contrast agent (Gadolinium), which improves the visibility of abnormalities.\n",
    "3. **T2-weighted (T2):** This modality highlights the fluid content within brain tissues.\n",
    "4. **T2-FLAIR (T2 - Fluid Attenuated Inversion Recovery):** This technique suppresses the fluid signals, making it easier to identify lesions that may not be visible on T1 or T2 images. It is particularly useful for detecting lesions in the brain's white matter, which are challenging to spot with other scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting slice 95 from all image arrays for subject 355\n",
    "slice = 95\n",
    "print(\"Slice Number: \" + str(slice))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# T1\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(test_image_t1[:,:,slice], cmap='gray')\n",
    "plt.title('T1')\n",
    "# T1ce\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(test_image_t1ce[:,:,slice], cmap='gray')\n",
    "plt.title('T1ce')\n",
    "# T2\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(test_image_t2[:,:,slice], cmap='gray')\n",
    "plt.title('T2')\n",
    "# Flair\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(test_image_flair[:,:,slice], cmap='gray')\n",
    "plt.title('FLAIR')\n",
    "# Mask\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(test_image_seg[:,:,slice])\n",
    "plt.title('Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the modalities and segmentations have 3 dimensions. Each dimension consists of a series of two-dimensional images, known as slices, which all contain the same number of pixels and are stacked together to create this 3D representation. For instance, in our previous example, we displayed the 95th slice of a certain dimension.\n",
    "\n",
    "These 3 dimensions correspond to the three spatial dimensions of the image: width, height, and depth. In medical imaging, these dimensions are referred to as the axial (transverse), coronal, and sagittal planes, corresponding to the three main orientations of the human body and, therefore, the human brain.\n",
    "\n",
    "Here is a quick presentation of these 3 planes:\n",
    "1. **Axial (Transverse) Plane:** This plane divides the body into upper and lower parts and is perpendicular to the long axis of the body. In brain imaging, an axial slice is a horizontal cut through the brain.\n",
    "2. **Coronal (Frontal) Plane:** This plane divides the body into front (anterior) and back (posterior) parts. A coronal slice in brain imaging is a vertical cut from one side of the head to the other, dividing the brain into front and back sections.\n",
    "3. **Sagittal (Lateral) Plane:** This plane divides the body into left and right parts. A sagittal slice in brain imaging is a vertical cut from front to back, dividing the brain into left and right sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modality shape\n",
    "print(\"Modality: \", test_image_t1.shape)\n",
    "# Segmentation shape\n",
    "print(\"Segmentation: \", test_image_seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 9th slice of T1 array in all three planes\n",
    "slice = 95\n",
    "print(\"Slice number: \" + str(slice))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Apply a 90Â° rotation with an automatic resizing, otherwise the display is less obvious to analyze\n",
    "# T1 - Transverse View\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(test_image_t1ce[:,:,slice], cmap='gray')\n",
    "plt.title('T1 - Transverse View')\n",
    "# T1 - Frontal View\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(rotate(test_image_t1ce[:,slice,:], 90, resize=True), cmap='gray')\n",
    "plt.title('T1 - Frontal View')\n",
    "# T1 - Sagittal View\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(rotate(test_image_t1ce[slice,:,:], 90, resize=True), cmap='gray')\n",
    "plt.title('T1 - Sagittal View')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Splitting the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of directories with studies\n",
    "train_and_val_directories = [f.path for f in os.scandir(Train_Path) if f.is_dir()]\n",
    "\n",
    "def pathListIntoIds(dirList):\n",
    "    x = []\n",
    "    for i in range(0,len(dirList)):\n",
    "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
    "    return x\n",
    "\n",
    "train_and_test_ids = pathListIntoIds(train_and_val_directories);\n",
    "\n",
    "# Change distributions for different applications\n",
    "train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2)\n",
    "train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15)\n",
    "\n",
    "# Print data distribution (Train: 68%, Test: 12%, Val: 20%)\n",
    "print(f\"Train length: {len(train_ids)}\")\n",
    "print(f\"Validation length: {len(val_ids)}\")\n",
    "print(f\"Test length: {len(test_ids)}\")\n",
    "plt.bar([\"Train\",\"Valid\",\"Test\"],\n",
    "        [len(train_ids), len(val_ids), len(test_ids)],\n",
    "        align='center',\n",
    "        color=[ 'green','red', 'blue'],\n",
    "        label=[\"Train\", \"Valid\", \"Test\"]\n",
    "       )\n",
    "plt.legend()\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Data Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DataGenerator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a neural network for image segmentation, we need both raw image data (X) and ground truth segmentations (y). This allows the network to learn tumor patterns and make accurate predictions from patient scans. However, directly loading all 3D images can overload system memory and cause shape mismatch errors. Therefore, we use a Data Generator for image preprocessing, which includes several steps:\n",
    "1. **Retrieve paths:** Obtain the paths for the T1CE and FLAIR modalities (for complementary anatomical and tissue contrast information) and the ground truth segmentation.\n",
    "2. **Load data:** Load the selected slices (60-135) from these modalities and the corresponding segmentation.\n",
    "3. **Create arrays:** Form X arrays with the slices from T1CE and FLAIR, and y arrays with the segmentation slices.\n",
    "4. **Class reassignment:** Assign a value of 3 to all instances of 4 in the mask array to correct the missing class issue.\n",
    "\n",
    "Additional preprocessing steps include:\n",
    "* **Axial plane:** Use the axial plane for its square shape (240x240). This enables visualizing predictions across all planes without impact.\n",
    "* **One-Hot Encoding:** Apply One-Hot Encoding to the y array to convert classes (0 to 3) into a numerical format suitable for neural networks, avoiding any implied hierarchy between classes.\n",
    "* **Resize images:** Resize each slice from (240x240) to (128x128). This shape is chosen because it is a power of two, fitting well with pooling layers (MaxPooling2D) in CNNs, and balances computational efficiency and information preservation.\n",
    "\n",
    "While resizing to (256x256) could retain more detail, it significantly increases training time and memory usage.\n",
    "\n",
    "**Data Generator:** Utilized to process and send data to the neural network without overloading memory.\n",
    "\n",
    "**Epoch Handling:** For each epoch, the model processes 250 samples from the training dataset.\n",
    "\n",
    "**Sample Analysis:** Each sample consists of 150 slices (100 slices each from two modalities) resized to (128, 128).\n",
    "\n",
    "**Data Shapes:**\n",
    "* **X Array:** Shape (128, 128, 100, 2) for input images.\n",
    "* **Ground Truth (y):** One-Hot encoded segmentation with shape (100, 128, 128, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seg-areas\n",
    "SEGMENT_CLASSES = {\n",
    "    0 : 'NOT tumor',\n",
    "    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n",
    "    2 : 'EDEMA',\n",
    "    3 : 'ENHANCING' # original 4 -> converted into 3\n",
    "}\n",
    "\n",
    "# Select Slices and Image Size\n",
    "VOLUME_SLICES = 100\n",
    "VOLUME_START_AT = 22 # first slice of volume that we will include\n",
    "IMG_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(Batch_ids)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, Batch_ids):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
    "        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n",
    "        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n",
    "\n",
    "\n",
    "        # Generate data\n",
    "        for c, i in enumerate(Batch_ids):\n",
    "            case_path = os.path.join(Train_Path, i)\n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_flair.nii');\n",
    "            flair = nib.load(data_path).get_fdata()\n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n",
    "            t1ce = nib.load(data_path).get_fdata()\n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_seg.nii');\n",
    "            seg = nib.load(data_path).get_fdata()\n",
    "\n",
    "            for j in range(VOLUME_SLICES):\n",
    "                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(t1ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "\n",
    "                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n",
    "\n",
    "        # Generate masks\n",
    "        y[y==4] = 3;\n",
    "        mask = tf.one_hot(y, 4);\n",
    "        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n",
    "        return X/np.max(X), Y\n",
    "\n",
    "training_generator = DataGenerator(train_ids)\n",
    "valid_generator = DataGenerator(val_ids)\n",
    "test_generator = DataGenerator(test_ids)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
